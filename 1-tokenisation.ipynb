{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenisation\n",
    "\n",
    "In deze notebook zijn de commentaren van de enquête ingelezen in en wordt tokenisation toegepast: het splitsen van de tekst in aparte lemma's. Daarnaast worden ook de sentimentwaarden uit Speed en Brysbaert en de demografische factoren uit de canonenquête toegevoegd aan de lemma's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Imports\n",
    "\n",
    "Hieronder worden alle gebruikte modules opgesomd en geladen om de lemma's uit de canonenquête te trekken. Hiervoor wordt vooral gebruikt gemaakt van spaCy en Pandas.\n",
    "\n",
    "[spaCy](https://spacy.io/) is een bekende package om tekst te parsen en verwerken. Aan de hand van spaCy is het mogelijk om de woorden op te splitsen en te lemmatiseren. Dit onderzoek maakt gebruik van Bram Vanroys [load_spacy](https://github.com/BramVanroy/spacy_download) dat automatisch het model downloadt.\n",
    "\n",
    "[pandas](https://pandas.pydata.org/docs/index.html) is een package die gebruikt wordt in Python bij het verwerken van gegevens in tabellarische vorm. Hierdoor is het mogelijk om de CSV bestanden te laden, de lemma's eruit te halen en de verschillende DataFrames samen te brengen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.nl.Dutch at 0x1676f2e50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from spacy_download import load_spacy\n",
    "import os\n",
    "\n",
    "\n",
    "nlp = load_spacy(\"nl_core_news_lg\", exclude=[\"parser\", \"tagger\"])\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dataset inladen\n",
    "\n",
    "De dataset uit de canonenquête is opgekuist aan de hand van [OpenRefine](https://openrefine.org/) en wordt geladen in Python. Hier zijn ook al een aantal niet-Nederlandse woorden vertaald naar het Nederlands. Daarnaast zijn ook de antwoorden van niet-betrouwbare respondenten verwijderd die genormaliseerd zijn op 100 jaar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Data-Anon-cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze onderstaande code neemt alle open commentaren uit de enquête, verwerkt deze tot lemma's en linkt ze aan de desbetreffende persoon.\n",
    "Per lemma wordt het zinsnummer, lemmanummer, respondent-ID, part of speech en het woord in originele vorm bijgehouden.\n",
    "\n",
    "De canonenquête bevatte 7 toelichtingsvragen en 2 open vragen:\n",
    "- Q9: De aandacht voor de canon in voortgezet/secundair onderwijs.\n",
    "- Q10: Leerlingen in het voortgezet/secundair onderwijs moeten enkele klassiekers lezen.\n",
    "- Q11: Leerlingen in het voortgezet/secundair onderwijs moeten een elementaire kennis aangeboden krijgen van Nederlandstalige literatuurgeschiedenis\n",
    "- Q12: Een Nederlandstalige literaire canon moet streven naar genderdiversiteit.\n",
    "- Q14: Auteurs van verschillende culturele achtergronden moeten vertegenwoordigd zijn in de canon.\n",
    "- Q15: Er kunnen meerdere Nederlandstalige literaire canons naast elkaar bestaan.\n",
    "- Q16: Welke teksten uit de voormalige koloniale gebieden zouden in de canon moeten?\n",
    "- Q17: Welke teksten uit de kinder- en jeugdliteratuur zouden in de canon moeten?\n",
    "- QI: Heb je ideeën voor projecten die de canon in de kijker kunnen zetten?\n",
    "- QC: Eventuele opmerkingen\n",
    "\n",
    "Onderstaande code bundelt deze vragen samen waarbij nog steeds iedere vraag, user en sentence number herkenbaar zijn in de dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#De volgende code sorteert de open vragen uit de enquête en voegt ze toe aan een dictionary. \n",
    "ELAB = {question for question in df if \"ELAB\" in question}\n",
    "ELAB.add(\"QPERS_IDEAS\")\n",
    "ELAB.add(\"QPERS_COMPLAIN\")\n",
    "\n",
    "rows = [] # een lege lijst om te vullen met de lemma's\n",
    "\n",
    "#Via een function is het mogelijk om voor iedere open vraag dezelfde code te gebruiken waarin alles gecombineerd wordt\n",
    "def add_row(question_prefix, row, sentence_number, token_number, token):\n",
    "    identifier_prefix = question_prefix.replace('ELAB', '').replace('QPERS_IDEAS', 'QI_').replace('QPERS_COMPLAIN', 'QC_')\n",
    "    question_prefix2 = question_prefix.replace('_ELAB', '').replace('QPERS_IDEAS', 'QI').replace('QPERS_COMPLAIN', 'QC')\n",
    "    #new_row element bepaalt de structuur van hoe de lemma's opgeslaan worden\n",
    "    new_row = {\n",
    "        \"identifier\": identifier_prefix + row[\"ResponseId\"] + '_' + str(sentence_number) + '_' + str(token_number),\n",
    "        \"questions\": question_prefix2,\n",
    "        \"response_id\": row[\"ResponseId\"],\n",
    "        \"sentence_number\": sentence_number,\n",
    "        \"token_number\": token_number,\n",
    "        \"token\": token.text,\n",
    "        \"lemma\": token.lemma_,\n",
    "        \"pos\": token.pos_\n",
    "    }\n",
    "    rows.append(new_row)\n",
    "    \n",
    "#Onderstaande loop gaat over iedere vraag in de vooraf gedefinieerde ELAB-lijst met open-commentaarvragen\n",
    "for question in ELAB:\n",
    "    #Lege rijen worden met deze code overgeslaan\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isnull(row[str(question)]):\n",
    "            continue\n",
    "        #Via de NLP-module worden de lemma's uit het woord afgeleid\n",
    "        doc = nlp(row[str(question)])\n",
    "        for sentence_number, sentence in enumerate(doc.sents): #via enumerate worden het aantal zinnen per respondent bijgehouden\n",
    "            for token_number, token in enumerate (sentence):   #Hetzelfde wordt hier gedaan per woord\n",
    "                if \"ELAB\" in question or \"IDEAS\" in question or \"COMPLAIN\" in question:\n",
    "                    add_row(question, row, sentence_number, token_number, token)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "#Nadat het programma door alle vragen is gegaan, worden alle lemma's opgeslaan in een dataframe               \n",
    "large_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimentwaarden toevoegen aan de dataset\n",
    "\n",
    "De dataset van Speed en Brysbaert (2023) wordt geladen en gecombineerd aan de lemma's uit de canonenquête.\n",
    "Niet ieder lemma zal een match vinden in deze dataset, waardoor er dataverlies zal optreden. Het is ook niet nuttig om de woorden die geen sentimentwaarden hebben te integreren in de dataset. Hierdoor zal onderstaande linken op basis van het lemma dat terug te vinden is in Speed en Brysbaert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_df = pd.read_excel('data/SpeedBrysbaertEmotionNorms.xlsx')\n",
    "left_df = large_df\n",
    "\n",
    "merged_df = left_df.merge(right_df, left_on='lemma', right_on='Word', how='left')\n",
    "merged_df = merged_df.drop(columns=['Word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demografische gegevens toevoegen\n",
    "\n",
    "Via onderstaande code worden de geanonimiseerde demografische gegevens opnieuw gelinkt aan ieder lemma aan de hand van de response_id. Hierdoor is het mogelijk om te kijken of de demografische factoren een voorspellende factor kunnen hebben op de sentimentwaarden.\n",
    "\n",
    "Deze data worden dan opgeslagen in een CSV-document die dan opgeladen wordt in R om regressiemodellen te bouwen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/bbqr68nn3_j5swj_l880wx_40000gn/T/ipykernel_51325/3326229177.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  demographics_subset.rename(columns={'ResponseId': 'response_id'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/data_anon.csv') #de data van het CRR wordt geladen in een andere dataframe\n",
    "#via een subset dataframe worden enkel response_ID en de demografische gegevens geladen voor een snellere verwerking\n",
    "demographics_subset = df[['ResponseId', 'QPERS_GENDER', 'QPERS_LANG', 'QPERS_EDU', 'QPERS_LING_JOB', 'QPERS_BIRTH_COUNTRY', 'QPERS_READING', 'QPERS_AGE']]\n",
    "#de kolom van de originele data wordt hernoemd zodat deze overeenkomt met de vorige code.\n",
    "demographics_subset.rename(columns={'ResponseId': 'response_id'}, inplace=True)\n",
    "demo_merged_df = pd.merge(merged_df, demographics_subset, on='response_id')\n",
    "#Rijen waar er geen sentimentwaarden voor bestaan (denk aan \"een\") worden overgeslaan.\n",
    "if not os.path.exists('sorted_data'): #Dit maakt een map aan indien er geen zou bestaan\n",
    "    os.makedirs('sorted_data')\n",
    "merged_df_final = demo_merged_df.dropna()\n",
    "merged_df_final.to_csv('sorted_data/Combined_Ratings.csv') #de gecombineerde data wordt opgeslaan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
