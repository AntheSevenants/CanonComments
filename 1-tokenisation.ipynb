{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenisation\n",
    "\n",
    "In deze notebook laden we de commentaren van de enquête in en passen we tokenisation toe: het splitsen van de tekst in aparte woorden/entiteiten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Imports\n",
    "\n",
    "We laden uitbreidingen in die we nodig hebben om de dataset in te laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spacy_download import load_spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dataset inladen\n",
    "\n",
    "We laden de dataset in en tonen een preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Tokenisation\n",
    "\n",
    "[spaCy](https://spacy.io/) is een bekende package om tekst te parsen en verwerken. Met spaCy kan je héél veel (POS-taggen, tokenisen, lemmatisen, named entity recognition). Wij gaan gewoon alles opsplitsen in woordjes.\n",
    "\n",
    "Ik heb [load_spacy](https://github.com/BramVanroy/spacy_download) van collega Bram Vanroy gebruikt, omdat dit automatisch ook het model downloadt als je het nog niet hebt. Verder werkt alles zoals je zou verwachten van spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.nl.Dutch at 0x7f74323a01c0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = load_spacy(\"nl_core_news_lg\", exclude=[\"parser\", \"tagger\"])\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ik toon hoe je een zin kunt tokeniseren met spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Freek Freek PROPN\n",
      "0 Van Van PROPN\n",
      "0 de de PROPN\n",
      "0 Velde Velde PROPN\n",
      "0 neemt aannemen VERB\n",
      "0 deel deel NOUN\n",
      "0 aan aan ADP\n",
      "0 de de DET\n",
      "0 fantastische fantastisch ADJ\n",
      "0 taalkundequiz taalkundequiz NOUN\n",
      "0 . . PUNCT\n",
      "1 Ik ik PRON\n",
      "1 speel speel VERB\n",
      "1 graag graag ADV\n",
      "1 Minecraft Minecraft PROPN\n",
      "1 . . PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Freek Van de Velde neemt deel aan de fantastische taalkundequiz. Ik speel graag Minecraft.\")\n",
    "for sentence_number, sentence in enumerate(doc.sents):\n",
    "    for token in sentence:\n",
    "        print(sentence_number, token.text, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu is het de bedoeling dat we hetzelfde doen met de commentaren uit de enquête. Ik geef een voorzet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We itereren over elke rij van de dataset\n",
    "for index, row in df.iterrows():\n",
    "    # Je kunt een kolom opvragen door die te indexeren\n",
    "    # bv. row[\"Q9_ELAB\"]\n",
    "\n",
    "    # Soms is er niks ingevuld, dan is de waarde van de kolom \"nan\" of \"null\"\n",
    "    # pd.isnull() kan hiervoor testen (True/False)\n",
    "    print(row[\"Q9_ELAB\"])\n",
    "\n",
    "    # Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We willen voor elke token een aparte rij in onze dataset. Zoiets:\n",
    "\n",
    "|sentence_no|token|lemma|pos|\n",
    "|---|---|---|---|\n",
    "|0|Freek|Freek|PROPN|\n",
    "|0|neemt|nemen|VERB|\n",
    "|0|deel|deel|NOUN|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kunt als volgt een eigen dataset maken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "row = { \"sentence_no\": 0, \"token\": \"neemt\", \"lemma\": \"nemen\", \"pos\": \"VERB\" }\n",
    "rows.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je ziet hoe je deze snippet in de loop hierboven kunt zetten om zo een nieuwe dataset te maken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_no</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>neemt</td>\n",
       "      <td>nemen</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_no  token  lemma   pos\n",
       "0            0  neemt  nemen  VERB"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame.from_dict(rows)\n",
    "new_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
