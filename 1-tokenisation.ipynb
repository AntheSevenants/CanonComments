{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenisation\n",
    "\n",
    "In deze notebook laden we de commentaren van de enquête in en passen we tokenisation toe: het splitsen van de tekst in aparte woorden/entiteiten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Imports\n",
    "\n",
    "We laden uitbreidingen in die we nodig hebben om de dataset in te laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spacy_download import load_spacy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dataset inladen\n",
    "\n",
    "We laden de dataset in en tonen een preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data_anon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Progress</th>\n",
       "      <th>Duration (in seconds)</th>\n",
       "      <th>Finished</th>\n",
       "      <th>RecordedDate</th>\n",
       "      <th>ResponseId</th>\n",
       "      <th>UserLanguage</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2_1</th>\n",
       "      <th>Q2_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Q11num</th>\n",
       "      <th>Q11numopp</th>\n",
       "      <th>Q7binary</th>\n",
       "      <th>Q13dialoog</th>\n",
       "      <th>Q13educatief</th>\n",
       "      <th>Q13elitair</th>\n",
       "      <th>Q13ouderwets</th>\n",
       "      <th>Q13reflectie</th>\n",
       "      <th>Q13fundament</th>\n",
       "      <th>Q13anders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>1030</td>\n",
       "      <td>True</td>\n",
       "      <td>2022-05-19 01:49:27</td>\n",
       "      <td>R_2wjiSHblELDyZMY</td>\n",
       "      <td>NL</td>\n",
       "      <td>Ik ben zestien of ouder, besef dat mijn gegeve...</td>\n",
       "      <td>patricia de martelaere</td>\n",
       "      <td>connie palmen - de vriendschap</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Analoog</td>\n",
       "      <td>een aanzet tot dialoog over literatuur en lite...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>een aanzet tot reflectie en een bron voor debat</td>\n",
       "      <td>een fundament van een collectief cultureel geh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>252</td>\n",
       "      <td>True</td>\n",
       "      <td>2022-05-19 02:05:02</td>\n",
       "      <td>R_1I6pSyFmgXFtRjH</td>\n",
       "      <td>NL</td>\n",
       "      <td>Ik ben zestien of ouder, besef dat mijn gegeve...</td>\n",
       "      <td>De Laatste Dichters - Christine Otten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>een fundament van een collectief cultureel geh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>651</td>\n",
       "      <td>True</td>\n",
       "      <td>2022-05-19 02:05:56</td>\n",
       "      <td>R_2OVxUcnSQ8BFaqa</td>\n",
       "      <td>NL</td>\n",
       "      <td>Ik ben zestien of ouder, besef dat mijn gegeve...</td>\n",
       "      <td>Gangreen 1- Jef Geeraerts</td>\n",
       "      <td>De Zondvloed - Jeroen Brouwers</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Analoog</td>\n",
       "      <td>een aanzet tot dialoog over literatuur en lite...</td>\n",
       "      <td>een educatief instrument</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>een fundament van een collectief cultureel geh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>1901</td>\n",
       "      <td>True</td>\n",
       "      <td>2022-05-19 02:31:21</td>\n",
       "      <td>R_2b0CIkQZZJq6ey6</td>\n",
       "      <td>NL</td>\n",
       "      <td>Ik ben zestien of ouder, besef dat mijn gegeve...</td>\n",
       "      <td>De Kapellekensbaan - Louis Paul Boon</td>\n",
       "      <td>Max Havelaar - Multatuli</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Analoog</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>elitair en niet-inclusief</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>een fundament van een collectief cultureel geh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>440</td>\n",
       "      <td>True</td>\n",
       "      <td>2022-05-19 03:15:49</td>\n",
       "      <td>R_6M3bVJZ8xxgVL5T</td>\n",
       "      <td>NL</td>\n",
       "      <td>Ik ben zestien of ouder, besef dat mijn gegeve...</td>\n",
       "      <td>Van den vos Reynaerde</td>\n",
       "      <td>Max Havelaar</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Analoog</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>een fundament van een collectief cultureel geh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Progress  Duration (in seconds)  Finished         RecordedDate  \\\n",
       "0           1       100                   1030      True  2022-05-19 01:49:27   \n",
       "1           2       100                    252      True  2022-05-19 02:05:02   \n",
       "2           3       100                    651      True  2022-05-19 02:05:56   \n",
       "3           4       100                   1901      True  2022-05-19 02:31:21   \n",
       "4           5       100                    440      True  2022-05-19 03:15:49   \n",
       "\n",
       "          ResponseId UserLanguage  \\\n",
       "0  R_2wjiSHblELDyZMY           NL   \n",
       "1  R_1I6pSyFmgXFtRjH           NL   \n",
       "2  R_2OVxUcnSQ8BFaqa           NL   \n",
       "3  R_2b0CIkQZZJq6ey6           NL   \n",
       "4  R_6M3bVJZ8xxgVL5T           NL   \n",
       "\n",
       "                                                  Q1  \\\n",
       "0  Ik ben zestien of ouder, besef dat mijn gegeve...   \n",
       "1  Ik ben zestien of ouder, besef dat mijn gegeve...   \n",
       "2  Ik ben zestien of ouder, besef dat mijn gegeve...   \n",
       "3  Ik ben zestien of ouder, besef dat mijn gegeve...   \n",
       "4  Ik ben zestien of ouder, besef dat mijn gegeve...   \n",
       "\n",
       "                                    Q2_1                            Q2_2  ...  \\\n",
       "0                 patricia de martelaere  connie palmen - de vriendschap  ...   \n",
       "1  De Laatste Dichters - Christine Otten                             NaN  ...   \n",
       "2              Gangreen 1- Jef Geeraerts  De Zondvloed - Jeroen Brouwers  ...   \n",
       "3   De Kapellekensbaan - Louis Paul Boon        Max Havelaar - Multatuli  ...   \n",
       "4                  Van den vos Reynaerde                    Max Havelaar  ...   \n",
       "\n",
       "  Q11num Q11numopp Q7binary  \\\n",
       "0    2.0       0.0  Analoog   \n",
       "1    2.0       0.0      NaN   \n",
       "2    2.0       0.0  Analoog   \n",
       "3    2.0       0.0  Analoog   \n",
       "4    1.0       1.0  Analoog   \n",
       "\n",
       "                                          Q13dialoog  \\\n",
       "0  een aanzet tot dialoog over literatuur en lite...   \n",
       "1                                                  0   \n",
       "2  een aanzet tot dialoog over literatuur en lite...   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "               Q13educatief                 Q13elitair Q13ouderwets  \\\n",
       "0                         0                          0            0   \n",
       "1                         0                          0            0   \n",
       "2  een educatief instrument                          0            0   \n",
       "3                         0  elitair en niet-inclusief            0   \n",
       "4                         0                          0            0   \n",
       "\n",
       "                                      Q13reflectie  \\\n",
       "0  een aanzet tot reflectie en een bron voor debat   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                0   \n",
       "4                                                0   \n",
       "\n",
       "                                        Q13fundament Q13anders  \n",
       "0  een fundament van een collectief cultureel geh...         0  \n",
       "1  een fundament van een collectief cultureel geh...         0  \n",
       "2  een fundament van een collectief cultureel geh...         0  \n",
       "3  een fundament van een collectief cultureel geh...         0  \n",
       "4  een fundament van een collectief cultureel geh...         0  \n",
       "\n",
       "[5 rows x 89 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Tokenisation\n",
    "\n",
    "[spaCy](https://spacy.io/) is een bekende package om tekst te parsen en verwerken. Met spaCy kan je héél veel (POS-taggen, tokenisen, lemmatisen, named entity recognition). Wij gaan gewoon alles opsplitsen in woordjes.\n",
    "\n",
    "Ik heb [load_spacy](https://github.com/BramVanroy/spacy_download) van collega Bram Vanroy gebruikt, omdat dit automatisch ook het model downloadt als je het nog niet hebt. Verder werkt alles zoals je zou verwachten van spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.nl.Dutch at 0x1f02413af10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = load_spacy(\"nl_core_news_lg\", exclude=[\"parser\", \"tagger\"])\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ik toon hoe je een zin kunt tokeniseren met spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Freek Freek PROPN\n",
      "0 Van Van PROPN\n",
      "0 de de PROPN\n",
      "0 Velde Velde PROPN\n",
      "0 neemt toenemen VERB\n",
      "0 geen geen DET\n",
      "0 deel deel NOUN\n",
      "0 aan aan ADP\n",
      "0 de de DET\n",
      "0 fantastische fantastisch ADJ\n",
      "0 taalkundequiz taalkundequiz NOUN\n",
      "0 . . PUNCT\n",
      "1 Ik ik PRON\n",
      "1 speel speel VERB\n",
      "1 graag graag ADV\n",
      "1 Minecraft Minecraft PROPN\n",
      "1 . . PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Freek Van de Velde neemt geen deel aan de fantastische taalkundequiz. Ik speel graag Minecraft.\")\n",
    "for sentence_number, sentence in enumerate(doc.sents):\n",
    "    for token in sentence:\n",
    "        print(sentence_number, token.text, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu is het de bedoeling dat we hetzelfde doen met de commentaren uit de enquête. Ik geef een voorzet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "# We itereren over elke rij van de dataset\n",
    "for index, row in df.iterrows():\n",
    "    # pd.isnull wordt gebruikt om lege rijen weg te laten.\n",
    "    if pd.isnull(row[\"Q9_ELAB\"]):\n",
    "        continue\n",
    "    \n",
    "    #Iedere zin per respondent wordt opgesplitst en benummerd.\n",
    "    doc = nlp(row[\"Q9_ELAB\"])\n",
    "    for sentence_number, sentence in enumerate(doc.sents):\n",
    "        for token in sentence:\n",
    "    # Je kunt een kolom opvragen door die te indexeren\n",
    "    # Hier worden de zinnen opgesplitst per respondent.\n",
    "            new_row = { \"response_id\": row[\"ResponseId\"], \"sentence_no\": sentence_number, \"token\": token.text, \"lemma\": token.lemma_, \"pos\": token.pos_ }\n",
    "            rows.append(new_row)\n",
    "\n",
    "    # Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We willen voor elke token een aparte rij in onze dataset. Zoiets:\n",
    "\n",
    "|sentence_no|token|lemma|pos|\n",
    "|---|---|---|---|\n",
    "|0|Freek|Freek|PROPN|\n",
    "|0|neemt|nemen|VERB|\n",
    "|0|deel|deel|NOUN|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kunt als volgt een eigen dataset maken:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je ziet hoe je deze snippet in de loop hierboven kunt zetten om zo een nieuwe dataset te maken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_id</th>\n",
       "      <th>sentence_no</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R_2wjiSHblELDyZMY</td>\n",
       "      <td>0</td>\n",
       "      <td>zonder</td>\n",
       "      <td>zonder</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R_2wjiSHblELDyZMY</td>\n",
       "      <td>0</td>\n",
       "      <td>canon</td>\n",
       "      <td>canon</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R_2wjiSHblELDyZMY</td>\n",
       "      <td>0</td>\n",
       "      <td>kan</td>\n",
       "      <td>kunnen</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R_2wjiSHblELDyZMY</td>\n",
       "      <td>0</td>\n",
       "      <td>je</td>\n",
       "      <td>je</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R_2wjiSHblELDyZMY</td>\n",
       "      <td>0</td>\n",
       "      <td>niks</td>\n",
       "      <td>niks</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         response_id  sentence_no   token   lemma   pos\n",
       "0  R_2wjiSHblELDyZMY            0  zonder  zonder   ADP\n",
       "1  R_2wjiSHblELDyZMY            0   canon   canon  NOUN\n",
       "2  R_2wjiSHblELDyZMY            0     kan  kunnen   AUX\n",
       "3  R_2wjiSHblELDyZMY            0      je      je  PRON\n",
       "4  R_2wjiSHblELDyZMY            0    niks    niks  PRON"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame.from_dict(rows)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De voorgaande code werd aangepast om voor iedere open vraag een nieuwe output-file te maken waarin de open antwoorden gelemmatiseerd wordt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deze lijn sorteert alle toelichtingsvragen uit de dataset en creëert een variabele per open vraag.\n",
    "ELAB = {question for question in df if \"ELAB\" in question}\n",
    "ELAB.add(\"QPERS_IDEAS\")\n",
    "ELAB.add(\"QPERS_COMPLAIN\")\n",
    "# De loop wordt gebruikt om iedere vraag apart te analyseren.\n",
    "for question in ELAB:\n",
    "    # Door de rijen in de loop te steken, worden de rijen per vraag herschreven.\n",
    "    rows = []\n",
    "    # Hier worden steeds per vraag de niet-beantwoorde vragen eruit gefilterd.\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isnull(row[str(question)]):\n",
    "            continue\n",
    "        # De beantwoorde vragen worden via deze code gelemmatiseerd waarbij een identifyer, de gebruiker, de zin,\n",
    "        # het woord, het lemma en de part of speech neergeschreven worden in een dataset.\n",
    "        doc = nlp(row[str(question)])\n",
    "        for sentence_number, sentence in enumerate(doc.sents):\n",
    "            for token in sentence:\n",
    "                if \"ELAB\" in question:\n",
    "                    new_row = { \"identifyer\": question.replace('ELAB', '') + row[\"ResponseId\"] + '_' + str(sentence_number), \"response_id\": row[\"ResponseId\"], \"sentence_number\": sentence_number, \"token\": token.text, \"lemma\": token.lemma_, \"pos\": token.pos_ }\n",
    "                    rows.append(new_row)\n",
    "                elif \"IDEAS\" in question:\n",
    "                    new_row = { \"identifyer\": question.replace('QPERS_IDEAS', 'QI_') + row[\"ResponseId\"] + '_' +str(sentence_number), \"response_id\": row[\"ResponseId\"], \"sentence_number\": sentence_number, \"token\": token.text, \"lemma\": token.lemma_, \"pos\": token.pos_ }\n",
    "                    rows.append(new_row)\n",
    "                else:\n",
    "                    new_row = { \"identifyer\": question.replace('QPERS_COMPLAIN', 'QC_') + row[\"ResponseId\"] + '_' +str(sentence_number), \"response_id\": row[\"ResponseId\"], \"sentence_number\": sentence_number, \"token\": token.text, \"lemma\": token.lemma_, \"pos\": token.pos_ }\n",
    "                    rows.append(new_row)\n",
    "    # De loop maakt verschillende .csv-bestanden aan waarbij de lemmatisering van de vragen respectievelijk opgeslaan worden.\n",
    "    new_df = pd.DataFrame.from_dict(rows) \n",
    "    new_df.to_csv(\"sorted_data/\" + question + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De afzonderlijke documenten worden via deze code gecombineerd, aan de hand van de identifyer (bestaande uit question-number, ResponseId en sentence number) om de vragen uit elkaar te houden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not read file Q10_ELAB.csv because of error: [Errno 13] Permission denied: 'sorted_data/'\n",
      "Could not read file Q11_ELAB.csv because of error: [Errno 13] Permission denied: 'sorted_data/'\n",
      "Could not read file Q14_ELAB.csv because of error: [Errno 13] Permission denied: 'sorted_data/'\n",
      "Could not read file Q15_ELAB.csv because of error: [Errno 13] Permission denied: 'sorted_data/'\n",
      "Could not read file Q16_ELAB.csv because of error: [Errno 13] Permission denied: 'sorted_data/'\n",
      "Could not read file Q17_ELAB.csv because of error: [Errno 13] Permission denied: 'sorted_data/'\n",
      "Could not read file Q9_ELAB.csv because of error: [Errno 13] Permission denied: 'sorted_data/'\n",
      "Could not read file QPERS_COMPLAIN.csv because of error: [Errno 13] Permission denied: 'sorted_data/'\n",
      "Could not read file QPERS_IDEAS.csv because of error: [Errno 13] Permission denied: 'sorted_data/'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projects\\CanonComments\\1-tokenisation.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/CanonComments/1-tokenisation.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/CanonComments/1-tokenisation.ipynb#X36sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not read file \u001b[39m\u001b[39m{\u001b[39;00mcsv\u001b[39m}\u001b[39;00m\u001b[39m because of error: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Projects/CanonComments/1-tokenisation.ipynb#X36sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m big_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(df_list)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/CanonComments/1-tokenisation.ipynb#X36sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m big_df\u001b[39m.\u001b[39mto_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(c_path, \u001b[39m'\u001b[39m\u001b[39mCombined_ELAB.csv\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Projects\\CanonComments\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:380\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[39melif\u001b[39;00m copy \u001b[39mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    378\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 380\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    381\u001b[0m     objs,\n\u001b[0;32m    382\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    383\u001b[0m     ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[0;32m    384\u001b[0m     join\u001b[39m=\u001b[39;49mjoin,\n\u001b[0;32m    385\u001b[0m     keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[0;32m    386\u001b[0m     levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[0;32m    387\u001b[0m     names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    388\u001b[0m     verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[0;32m    389\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    390\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    391\u001b[0m )\n\u001b[0;32m    393\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Projects\\CanonComments\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:443\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverify_integrity \u001b[39m=\u001b[39m verify_integrity\n\u001b[0;32m    441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39m=\u001b[39m copy\n\u001b[1;32m--> 443\u001b[0m objs, keys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_clean_keys_and_objs(objs, keys)\n\u001b[0;32m    445\u001b[0m \u001b[39m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    446\u001b[0m ndims \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32mc:\\Projects\\CanonComments\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:505\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    502\u001b[0m     objs_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[0;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs_list) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 505\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    507\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     objs_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "c_path = 'sorted_data/'\n",
    "csv_all = os.listdir(c_path)\n",
    "df_list = []\n",
    "\n",
    "for csv in csv_all:\n",
    "    csv_path = os.path.join(c_path, csv)\n",
    "    try:\n",
    "        df = pd.read_csv(c_path)\n",
    "        df_list.append(df)\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            df = pd.read_csv(c_path, sep='/t', encoding='utf-16')\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read file {csv} because of error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read file {csv} because of error: {e}\")\n",
    "                            \n",
    "big_df = pd.concat(df_list)\n",
    "\n",
    "big_df.to_csv(os.path.join(c_path, 'Combined_ELAB.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De canonenquête bevatte 7 toelichtingsvragen:\n",
    "- Q9: De aandacht voor de canon in voortgezet/secundair onderwijs.\n",
    "- Q10: Leerlingen in het voortgezet/secundair onderwijs moeten enkele klassiekers lezen.\n",
    "- Q11: Leerlingen in het voortgezet/secundair onderwijs moeten een elementaire kennis aangeboden krijgen van Nederlandstalige literatuurgeschiedenis\n",
    "- Q14: Een Nederlandstalige literaire canon moet streven naar genderdiversiteit.\n",
    "- Q15: Er kunnen meerdere Nederlandstalige literaire canons naast elkaar bestaan.\n",
    "- Q16: Welke teksten uit de voormalige koloniale gebieden zouden in de canon moeten?\n",
    "- Q17: Welke teksten uit de kinder- en jeugdliteratuur zouden in de canon moeten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
